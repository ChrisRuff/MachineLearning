Activation functions for the outputs of the neurons( best is ReLU for image rec.)
Loss functions- Mean squared error
							- Cross entropy -- THIS IS BETTER THAN MEAN SQUARED ERROR
Back propagation -- COMPLEX -- MIT VIDEO EXPLAINS WELL
	This is used to update the neural network

Optimizers - Alias for back propagation -- WE WILL USE ADAM
	ADAM - Pretty good

Underfitting -- Too few nodes
Overfitting -- Too many nodes
Robust Fit -- Perfect amount of nodes

Fixing bad training
	Dropout-- If there are too many nodes then you can drop nodes during training
						removes un-used nodes implemented in the neural network and placed between dense layers
						also pass a rate which gives the rate that a node is dropped

Convolutional Neural Network
	- Replace dense layers 
	- Kernals -- 2D Array with values to apply weights to inputs to get the outputs
	- Neural network generates the kernal to help decipher the image

Pooling Layers
	- Replaces dense layers
	- Divides the image into pools to parse
	- Average Pooling  -- takes the average of pool
	- Max Pooling -- takes max value of pool 

To use previously made model you have to export it from tensor flow

Course by fast.ai
